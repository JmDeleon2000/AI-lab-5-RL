{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "action_space = env.action_space.n\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 #learning rate\n",
    "max_iterations = 100000\n",
    "gamma = 0.995\n",
    "\n",
    "total = 0\n",
    "tot_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "observation = [30, 30, 50, 50]\n",
    "array_win = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_array =  np.array([15,10,1,10])\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/array_win + magic_array\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.0003339965343475342\n",
      "Mean Reward: 0.016\n",
      "Time Average: 0.002742120981216431\n",
      "Mean Reward: 22.943\n",
      "Episode: 2000\n",
      "Time Average: 0.0032657458782196047\n",
      "Mean Reward: 21.939\n",
      "Time Average: 0.0015198793411254883\n",
      "Mean Reward: 21.815\n",
      "Episode: 4000\n",
      "Time Average: 0.002586453676223755\n",
      "Mean Reward: 22.592\n",
      "Time Average: 0.0016990513801574706\n",
      "Mean Reward: 22.174\n",
      "Episode: 6000\n",
      "Time Average: 0.002977961540222168\n",
      "Mean Reward: 21.749\n",
      "Time Average: 0.0015129806995391845\n",
      "Mean Reward: 22.302\n",
      "Episode: 8000\n",
      "Time Average: 0.0017751960754394532\n",
      "Mean Reward: 21.616\n",
      "Time Average: 0.0022729485034942627\n",
      "Mean Reward: 22.378\n",
      "Episode: 10000\n",
      "Time Average: 0.0026828856468200683\n",
      "Mean Reward: 22.524\n",
      "Time Average: 0.0018529932498931884\n",
      "Mean Reward: 22.637\n",
      "Episode: 12000\n",
      "Time Average: 0.002387116193771362\n",
      "Mean Reward: 23.572\n",
      "Time Average: 0.002303032636642456\n",
      "Mean Reward: 24.68\n",
      "Epsilon: 0.8394533480303666\n",
      "Episode: 14000\n",
      "Time Average: 0.002741000175476074\n",
      "Mean Reward: 26.727\n",
      "Epsilon: 0.7985117269685725\n",
      "Time Average: 0.0018868052959442139\n",
      "Mean Reward: 28.231\n",
      "Epsilon: 0.7595669010105212\n",
      "Episode: 16000\n",
      "Time Average: 0.002332961559295654\n",
      "Mean Reward: 30.848\n",
      "Epsilon: 0.7225214829355084\n",
      "Time Average: 0.0022259700298309327\n",
      "Mean Reward: 31.626\n",
      "Epsilon: 0.687282835269431\n",
      "Episode: 18000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.0028225805759429933\n",
      "Mean Reward: 34.23\n",
      "Epsilon: 0.6537628386312633\n",
      "Time Average: 0.0024430041313171385\n",
      "Mean Reward: 36.835\n",
      "Epsilon: 0.6218776713776856\n",
      "Episode: 20000\n",
      "Epsilon: 0.606523077874078\n",
      "Time Average: 0.003920328617095947\n",
      "Mean Reward: 39.686\n",
      "Epsilon: 0.5769418771107269\n",
      "Time Average: 0.004390990018844605\n",
      "Mean Reward: 43.917\n",
      "Epsilon: 0.5626967797130051\n",
      "Episode: 22000\n",
      "Epsilon: 0.5488034037068503\n",
      "Time Average: 0.004358715057373047\n",
      "Mean Reward: 47.995\n",
      "Epsilon: 0.5352530648457575\n",
      "Epsilon: 0.5220372933033263\n",
      "Time Average: 0.003398024559020996\n",
      "Mean Reward: 52.75\n",
      "Epsilon: 0.5091478283790776\n",
      "Episode: 24000\n",
      "Time Average: 0.004377699851989746\n",
      "Mean Reward: 58.212\n",
      "Epsilon: 0.484315790359524\n",
      "Time Average: 0.004008962631225586\n",
      "Mean Reward: 62.948\n",
      "Episode: 26000\n",
      "Epsilon: 0.44931997732828616\n",
      "Time Average: 0.006670331239700318\n",
      "Mean Reward: 72.196\n",
      "Epsilon: 0.43822595366018774\n",
      "Epsilon: 0.4274058491752072\n",
      "Time Average: 0.005102030038833618\n",
      "Mean Reward: 76.869\n",
      "Epsilon: 0.41685290061763824\n",
      "Episode: 28000\n",
      "Time Average: 0.006159999370574951\n",
      "Mean Reward: 80.473\n",
      "Epsilon: 0.396522249086328\n",
      "Time Average: 0.005405939817428589\n",
      "Mean Reward: 84.247\n",
      "Epsilon: 0.3771831593051582\n",
      "Episode: 30000\n",
      "Epsilon: 0.3678702439938449\n",
      "Time Average: 0.011199517488479614\n",
      "Mean Reward: 97.952\n",
      "Epsilon: 0.3499285630596461\n",
      "Time Average: 0.008894954442977904\n",
      "Mean Reward: 109.414\n",
      "Epsilon: 0.3412885827413639\n",
      "Episode: 32000\n",
      "Time Average: 0.009525036573410034\n",
      "Mean Reward: 117.497\n",
      "Epsilon: 0.31662766589938623\n",
      "Time Average: 0.00762899374961853\n",
      "Mean Reward: 121.496\n",
      "Epsilon: 0.30880990796138097\n",
      "Episode: 34000\n",
      "Time Average: 0.010760919570922851\n",
      "Mean Reward: 128.883\n",
      "Epsilon: 0.29374870383187524\n",
      "Time Average: 0.010691993951797485\n",
      "Mean Reward: 143.574\n",
      "Episode: 36000\n",
      "Epsilon: 0.27252293559946306\n",
      "Time Average: 0.015729596614837648\n",
      "Mean Reward: 154.84\n",
      "Epsilon: 0.2657941542182801\n",
      "Epsilon: 0.25923151114313064\n",
      "Time Average: 0.010419980764389039\n",
      "Mean Reward: 167.139\n",
      "Epsilon: 0.2528309043033471\n",
      "Episode: 38000\n",
      "Time Average: 0.01490728759765625\n",
      "Mean Reward: 172.314\n",
      "Epsilon: 0.23456178479157042\n",
      "Time Average: 0.01161659860610962\n",
      "Mean Reward: 176.224\n",
      "Epsilon: 0.22877029070403326\n",
      "Episode: 40000\n",
      "Epsilon: 0.22312179264543486\n",
      "Time Average: 0.012904840230941772\n",
      "Mean Reward: 198.877\n",
      "Epsilon: 0.21223974910298996\n",
      "Time Average: 0.009418829441070557\n",
      "Mean Reward: 220.191\n",
      "Epsilon: 0.206999401647574\n",
      "Episode: 42000\n",
      "Epsilon: 0.20188844202629158\n",
      "Time Average: 0.02030530023574829\n",
      "Mean Reward: 229.925\n",
      "Epsilon: 0.192041986461381\n",
      "Time Average: 0.009713387966156006\n",
      "Mean Reward: 229.294\n",
      "Epsilon: 0.18730033585474753\n",
      "Episode: 44000\n",
      "Epsilon: 0.18267575990917997\n",
      "Time Average: 0.0211877760887146\n",
      "Mean Reward: 281.704\n",
      "Epsilon: 0.17816536796962992\n",
      "Time Average: 0.011105172872543334\n",
      "Mean Reward: 257.508\n",
      "Episode: 46000\n",
      "Time Average: 0.013358653306961059\n",
      "Mean Reward: 231.064\n",
      "Time Average: 0.00971106743812561\n",
      "Mean Reward: 225.294\n",
      "Episode: 48000\n",
      "Epsilon: 0.1495615146451681\n",
      "Time Average: 0.017143824577331542\n",
      "Mean Reward: 289.266\n",
      "Time Average: 0.014005993843078613\n",
      "Mean Reward: 330.694\n",
      "Episode: 50000\n",
      "Time Average: 0.01708470344543457\n",
      "Mean Reward: 321.264\n",
      "Epsilon: 0.12872830587316755\n",
      "Time Average: 0.014186942100524902\n",
      "Mean Reward: 335.277\n",
      "Episode: 52000\n",
      "Time Average: 0.01587385129928589\n",
      "Mean Reward: 315.537\n",
      "Epsilon: 0.11942662334734862\n",
      "Epsilon: 0.11647789670960881\n",
      "Time Average: 0.011787271738052368\n",
      "Mean Reward: 274.04\n",
      "Episode: 54000\n",
      "Time Average: 0.018824729919433594\n",
      "Mean Reward: 341.344\n",
      "Epsilon: 0.10806140735150761\n",
      "Epsilon: 0.10539329582463075\n",
      "Time Average: 0.011499145269393921\n",
      "Mean Reward: 353.692\n",
      "Episode: 56000\n",
      "Epsilon: 0.10025307881289336\n",
      "Time Average: 0.02375084185600281\n",
      "Mean Reward: 379.442\n",
      "Time Average: 0.010740013599395751\n",
      "Mean Reward: 330.884\n",
      "Epsilon: 0.09300896645525675\n",
      "Episode: 58000\n",
      "Time Average: 0.02362514615058899\n",
      "Mean Reward: 399.189\n",
      "Time Average: 0.014374822854995728\n",
      "Mean Reward: 409.998\n",
      "Episode: 60000\n",
      "Epsilon: 0.08207986830082019\n",
      "Time Average: 0.02175153923034668\n",
      "Mean Reward: 363.394\n",
      "Time Average: 0.014187998294830322\n",
      "Mean Reward: 407.912\n",
      "Epsilon: 0.07614892039067295\n",
      "Episode: 62000\n",
      "Time Average: 0.016431759119033815\n",
      "Mean Reward: 363.811\n",
      "Epsilon: 0.07064653241661088\n",
      "Time Average: 0.010308984279632569\n",
      "Mean Reward: 312.515\n",
      "Episode: 64000\n",
      "Time Average: 0.011602871179580688\n",
      "Mean Reward: 265.927\n",
      "Epsilon: 0.06554173738624092\n",
      "Time Average: 0.012119997978210449\n",
      "Mean Reward: 330.925\n",
      "Episode: 66000\n",
      "Time Average: 0.026260234594345094\n",
      "Mean Reward: 444.104\n",
      "Epsilon: 0.059304468163859834\n",
      "Time Average: 0.015522997856140136\n",
      "Mean Reward: 453.105\n",
      "Episode: 68000\n",
      "Time Average: 0.025555294752120972\n",
      "Mean Reward: 439.909\n",
      "Time Average: 0.013138552188873292\n",
      "Mean Reward: 397.294\n",
      "Epsilon: 0.051043637365901166\n",
      "Episode: 70000\n",
      "Time Average: 0.02101234459877014\n",
      "Mean Reward: 389.921\n",
      "Time Average: 0.009827002763748168\n",
      "Mean Reward: 305.331\n",
      "Episode: 72000\n",
      "Time Average: 0.017381639719009398\n",
      "Mean Reward: 342.254\n",
      "Time Average: 0.014518720626831055\n",
      "Mean Reward: 437.75\n",
      "Episode: 74000\n",
      "Time Average: 0.026021804094314576\n",
      "Mean Reward: 469.404\n",
      "Time Average: 0.015568034172058105\n",
      "Mean Reward: 461.421\n",
      "Episode: 76000\n",
      "Time Average: 0.02554426956176758\n",
      "Mean Reward: 464.464\n",
      "Time Average: 0.016300638198852538\n",
      "Mean Reward: 469.313\n",
      "Episode: 78000\n",
      "Time Average: 0.026170814275741576\n",
      "Mean Reward: 463.304\n",
      "Time Average: 0.014499133110046386\n",
      "Mean Reward: 445.336\n",
      "Episode: 80000\n",
      "Time Average: 0.0273201060295105\n",
      "Mean Reward: 429.081\n",
      "Time Average: 0.014089986085891723\n",
      "Mean Reward: 431.818\n",
      "Episode: 82000\n",
      "Time Average: 0.02427968621253967\n",
      "Mean Reward: 431.561\n",
      "Time Average: 0.012802993059158324\n",
      "Mean Reward: 388.637\n",
      "Episode: 84000\n",
      "Time Average: 0.024552701711654662\n",
      "Mean Reward: 436.853\n",
      "Time Average: 0.01774393892288208\n",
      "Mean Reward: 455.155\n",
      "Episode: 86000\n",
      "Time Average: 0.0255836021900177\n",
      "Mean Reward: 455.624\n",
      "Time Average: 0.014675965547561646\n",
      "Mean Reward: 440.721\n",
      "Episode: 88000\n",
      "Time Average: 0.024442727088928223\n",
      "Mean Reward: 453.52\n",
      "Time Average: 0.015550967693328857\n",
      "Mean Reward: 474.947\n",
      "Episode: 90000\n",
      "Time Average: 0.025399558305740356\n",
      "Mean Reward: 469.104\n",
      "Time Average: 0.014492966413497925\n",
      "Mean Reward: 443.665\n",
      "Episode: 92000\n",
      "Time Average: 0.024439189910888674\n",
      "Mean Reward: 427.339\n",
      "Time Average: 0.013720523595809936\n",
      "Mean Reward: 417.012\n",
      "Episode: 94000\n",
      "Time Average: 0.022809591770172118\n",
      "Mean Reward: 398.956\n",
      "Time Average: 0.013006002902984619\n",
      "Mean Reward: 394.321\n",
      "Episode: 96000\n",
      "Time Average: 0.01306449556350708\n",
      "Mean Reward: 325.304\n",
      "Time Average: 0.015594050407409668\n",
      "Mean Reward: 467.984\n",
      "Episode: 98000\n",
      "Time Average: 0.028369311094284057\n",
      "Mean Reward: 474.271\n",
      "Time Average: 0.015702032566070556\n",
      "Mean Reward: 469.893\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_iterations):\n",
    "    t0 = time.time()\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    if (i % 2000 == 0):\n",
    "        print(f\"Episode: {i}\")\n",
    "    \n",
    "    while(not(done)):\n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space)\n",
    "\n",
    "        new_state, step_reward, done, info = env.step(action)\n",
    "\n",
    "        reward += step_reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if (i % 2000 == 0):\n",
    "            env.render()\n",
    "        \n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            \n",
    "            new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "        \n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: \n",
    "        if reward > prev_reward and i > 10000:\n",
    "            epsilon = math.pow(epsilon_decay, i - 10000)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    iteration_time = t1-t0\n",
    "    total += iteration_time\n",
    "\n",
    "    tot_reward += reward\n",
    "    prev_reward = reward\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = tot_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        tot_reward = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
