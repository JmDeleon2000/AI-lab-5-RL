{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "action_space = env.action_space.n\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 #learning rate\n",
    "max_iterations = 100000\n",
    "gamma = 0.95\n",
    "\n",
    "total = 0\n",
    "tot_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "observation = [30, 30, 50, 50]\n",
    "array_win = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_array =  np.array([15,10,1,10])\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/array_win + magic_array\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.00023599696159362792\n",
      "Mean Reward: 0.01\n",
      "Time Average: 0.002102958679199219\n",
      "Mean Reward: 22.319\n",
      "Episode: 2000\n",
      "Time Average: 0.001750030517578125\n",
      "Mean Reward: 22.368\n",
      "Time Average: 0.0016110219955444336\n",
      "Mean Reward: 21.882\n",
      "Episode: 4000\n",
      "Time Average: 0.0020630314350128173\n",
      "Mean Reward: 21.989\n",
      "Time Average: 0.0020600337982177733\n",
      "Mean Reward: 21.962\n",
      "Episode: 6000\n",
      "Time Average: 0.0027570176124572755\n",
      "Mean Reward: 22.066\n",
      "Time Average: 0.0016519687175750733\n",
      "Mean Reward: 22.757\n",
      "Episode: 8000\n",
      "Time Average: 0.0020181529521942137\n",
      "Mean Reward: 22.099\n",
      "Time Average: 0.0027918999195098875\n",
      "Mean Reward: 22.183\n",
      "Episode: 10000\n",
      "Time Average: 0.0020263235569000244\n",
      "Mean Reward: 22.053\n",
      "Time Average: 0.0033259568214416505\n",
      "Mean Reward: 43.83\n",
      "Episode: 12000\n",
      "Time Average: 0.003584658145904541\n",
      "Mean Reward: 46.437\n",
      "Time Average: 0.0031869943141937256\n",
      "Mean Reward: 47.413\n",
      "Episode: 14000\n",
      "Time Average: 0.005552551984786987\n",
      "Mean Reward: 46.685\n",
      "Time Average: 0.003259023189544678\n",
      "Mean Reward: 47.644\n",
      "Episode: 16000\n",
      "Time Average: 0.005105007886886597\n",
      "Mean Reward: 47.6\n",
      "Time Average: 0.004760027885437012\n",
      "Mean Reward: 48.178\n",
      "Episode: 18000\n",
      "Time Average: 0.004356469392776489\n",
      "Mean Reward: 48.778\n",
      "Time Average: 0.005108997583389282\n",
      "Mean Reward: 48.667\n",
      "Episode: 20000\n",
      "Time Average: 0.004177011966705322\n",
      "Mean Reward: 50.377\n",
      "Time Average: 0.003116871118545532\n",
      "Mean Reward: 49.041\n",
      "Episode: 22000\n",
      "Time Average: 0.003673290252685547\n",
      "Mean Reward: 49.324\n",
      "Time Average: 0.0029749150276184083\n",
      "Mean Reward: 46.851\n",
      "Episode: 24000\n",
      "Time Average: 0.005026970863342285\n",
      "Mean Reward: 46.477\n",
      "Time Average: 0.004283917665481567\n",
      "Mean Reward: 45.177\n",
      "Episode: 26000\n",
      "Time Average: 0.0037408621311187744\n",
      "Mean Reward: 44.055\n",
      "Time Average: 0.004608043909072876\n",
      "Mean Reward: 44.619\n",
      "Episode: 28000\n",
      "Time Average: 0.004537012338638306\n",
      "Mean Reward: 45.426\n",
      "Time Average: 0.0028619914054870605\n",
      "Mean Reward: 45.041\n",
      "Episode: 30000\n",
      "Time Average: 0.0040660881996154785\n",
      "Mean Reward: 45.537\n",
      "Time Average: 0.0036820566654205324\n",
      "Mean Reward: 45.256\n",
      "Episode: 32000\n",
      "Time Average: 0.003688615798950195\n",
      "Mean Reward: 44.729\n",
      "Time Average: 0.003171571731567383\n",
      "Mean Reward: 45.937\n",
      "Episode: 34000\n",
      "Time Average: 0.0038609063625335695\n",
      "Mean Reward: 45.385\n",
      "Time Average: 0.003154989242553711\n",
      "Mean Reward: 45.201\n",
      "Episode: 36000\n",
      "Time Average: 0.004864436388015747\n",
      "Mean Reward: 43.699\n",
      "Time Average: 0.002972996473312378\n",
      "Mean Reward: 43.398\n",
      "Episode: 38000\n",
      "Time Average: 0.0035699970722198487\n",
      "Mean Reward: 42.672\n",
      "Time Average: 0.0026900014877319334\n",
      "Mean Reward: 41.972\n",
      "Episode: 40000\n",
      "Time Average: 0.003931511163711548\n",
      "Mean Reward: 41.469\n",
      "Time Average: 0.00261600136756897\n",
      "Mean Reward: 41.314\n",
      "Episode: 42000\n",
      "Time Average: 0.003485429048538208\n",
      "Mean Reward: 38.991\n",
      "Time Average: 0.0024045631885528562\n",
      "Mean Reward: 36.18\n",
      "Episode: 44000\n",
      "Time Average: 0.003377032041549683\n",
      "Mean Reward: 37.707\n",
      "Time Average: 0.0032839994430541994\n",
      "Mean Reward: 37.586\n",
      "Episode: 46000\n",
      "Time Average: 0.004953970670700073\n",
      "Mean Reward: 40.317\n",
      "Time Average: 0.002767950773239136\n",
      "Mean Reward: 39.808\n",
      "Episode: 48000\n",
      "Time Average: 0.0040142662525177\n",
      "Mean Reward: 42.172\n",
      "Time Average: 0.002419001579284668\n",
      "Mean Reward: 39.609\n",
      "Episode: 50000\n",
      "Time Average: 0.003715384244918823\n",
      "Mean Reward: 39.466\n",
      "Time Average: 0.003868934631347656\n",
      "Mean Reward: 39.325\n",
      "Episode: 52000\n",
      "Time Average: 0.0029601767063140868\n",
      "Mean Reward: 39.112\n",
      "Time Average: 0.0023089983463287353\n",
      "Mean Reward: 36.547\n",
      "Episode: 54000\n",
      "Time Average: 0.0026702251434326174\n",
      "Mean Reward: 36.872\n",
      "Time Average: 0.0025570414066314697\n",
      "Mean Reward: 39.219\n",
      "Episode: 56000\n",
      "Time Average: 0.003085251808166504\n",
      "Mean Reward: 40.025\n",
      "Time Average: 0.0024142754077911377\n",
      "Mean Reward: 39.08\n",
      "Episode: 58000\n",
      "Time Average: 0.002929006338119507\n",
      "Mean Reward: 38.4\n",
      "Time Average: 0.0022439942359924318\n",
      "Mean Reward: 36.475\n",
      "Episode: 60000\n",
      "Time Average: 0.003140016794204712\n",
      "Mean Reward: 38.375\n",
      "Time Average: 0.002401965856552124\n",
      "Mean Reward: 35.2\n",
      "Episode: 62000\n",
      "Time Average: 0.003376832962036133\n",
      "Mean Reward: 36.164\n",
      "Time Average: 0.003268031120300293\n",
      "Mean Reward: 35.829\n",
      "Episode: 64000\n",
      "Time Average: 0.0032624142169952394\n",
      "Mean Reward: 36.114\n",
      "Time Average: 0.0024189248085021973\n",
      "Mean Reward: 36.904\n",
      "Episode: 66000\n",
      "Time Average: 0.004307973861694336\n",
      "Mean Reward: 36.996\n",
      "Time Average: 0.00251300048828125\n",
      "Mean Reward: 38.515\n",
      "Episode: 68000\n",
      "Time Average: 0.0028892064094543455\n",
      "Mean Reward: 39.206\n",
      "Time Average: 0.002424518585205078\n",
      "Mean Reward: 37.665\n",
      "Episode: 70000\n",
      "Time Average: 0.003054152488708496\n",
      "Mean Reward: 38.861\n",
      "Time Average: 0.002398794412612915\n",
      "Mean Reward: 38.128\n",
      "Episode: 72000\n",
      "Time Average: 0.0026941916942596436\n",
      "Mean Reward: 38.675\n",
      "Time Average: 0.0024740478992462157\n",
      "Mean Reward: 37.393\n",
      "Episode: 74000\n",
      "Time Average: 0.0036043925285339355\n",
      "Mean Reward: 37.355\n",
      "Time Average: 0.0023569931983947754\n",
      "Mean Reward: 38.028\n",
      "Episode: 76000\n",
      "Time Average: 0.0031559338569641113\n",
      "Mean Reward: 37.966\n",
      "Time Average: 0.0028129873275756835\n",
      "Mean Reward: 38.289\n",
      "Episode: 78000\n",
      "Time Average: 0.002846691370010376\n",
      "Mean Reward: 37.687\n",
      "Time Average: 0.0025639936923980713\n",
      "Mean Reward: 38.365\n",
      "Episode: 80000\n",
      "Time Average: 0.002932811260223389\n",
      "Mean Reward: 37.207\n",
      "Time Average: 0.0023570625782012937\n",
      "Mean Reward: 36.958\n",
      "Episode: 82000\n",
      "Time Average: 0.003588037014007568\n",
      "Mean Reward: 35.931\n",
      "Time Average: 0.0033129582405090332\n",
      "Mean Reward: 37.68\n",
      "Episode: 84000\n",
      "Time Average: 0.003906964540481567\n",
      "Mean Reward: 38.889\n",
      "Time Average: 0.0023740406036376954\n",
      "Mean Reward: 37.19\n",
      "Episode: 86000\n",
      "Time Average: 0.0029335296154022216\n",
      "Mean Reward: 38.201\n",
      "Time Average: 0.0022009937763214113\n",
      "Mean Reward: 35.598\n",
      "Episode: 88000\n",
      "Time Average: 0.00256282901763916\n",
      "Mean Reward: 34.623\n",
      "Time Average: 0.002225001335144043\n",
      "Mean Reward: 34.669\n",
      "Episode: 90000\n",
      "Time Average: 0.002850792407989502\n",
      "Mean Reward: 35.148\n",
      "Time Average: 0.0023249664306640625\n",
      "Mean Reward: 36.428\n",
      "Episode: 92000\n",
      "Time Average: 0.0032684259414672854\n",
      "Mean Reward: 34.886\n",
      "Time Average: 0.002566986322402954\n",
      "Mean Reward: 33.574\n",
      "Episode: 94000\n",
      "Time Average: 0.0026143321990966797\n",
      "Mean Reward: 32.844\n",
      "Time Average: 0.002123960256576538\n",
      "Mean Reward: 34.683\n",
      "Episode: 96000\n",
      "Time Average: 0.002712029695510864\n",
      "Mean Reward: 31.721\n",
      "Time Average: 0.0021179041862487793\n",
      "Mean Reward: 31.268\n",
      "Episode: 98000\n",
      "Time Average: 0.0025454800128936767\n",
      "Mean Reward: 30.842\n",
      "Time Average: 0.002184330940246582\n",
      "Mean Reward: 31.71\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_iterations):\n",
    "    t0 = time.time()\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    if (i % 2000 == 0):\n",
    "        print(f\"Episode: {i}\")\n",
    "    \n",
    "    while(not(done)):\n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, action_space)\n",
    "\n",
    "        new_state, step_reward, done, info = env.step(action)\n",
    "\n",
    "        reward += step_reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if (i % 2000 == 0):\n",
    "            env.render()\n",
    "        \n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            \n",
    "            new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "        \n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.05: \n",
    "        if reward > prev_reward and i > 10000:\n",
    "            epsilon = math.pow(epsilon_decay, i - 10000)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    iteration_time = t1-t0\n",
    "    total += iteration_time\n",
    "\n",
    "    tot_reward += reward\n",
    "    prev_reward = reward\n",
    "\n",
    "    if i % 1000 == 0: \n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = tot_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        tot_reward = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
